{"cells":[{"cell_type":"markdown","metadata":{"id":"a0v9-lkx-S40"},"source":["# **Vision Transformer**\n","CODE:https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/vit_torch.py\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":121,"metadata":{"id":"FgTVi4Fb9Nvj"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn \n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import ToTensor\n","from torchvision.datasets.mnist import MNIST\n","\n","np.random.seed(0)\n","\n","def patchify (images, n_patches):\n","    n,c,h,w = images.shape              #50x1x28x28\n","    \n","    assert h==w,\"Patchify method is implemented for square images only\"   #if 28==28\n","    patches = torch.zeros(n,n_patches**2,(h//n_patches)**2)               #50x49x16\n","    patch_size = h//n_patches\n","    \n","    for idx, image in enumerate(images):\n","        for i in range (n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] # patching for one image \n","                patches[idx, i*n_patches+j]=patch.flatten()     # taking all (49) patches one by one and assigning the patch , first image first patch = 1x16, first image second patch =1x16\n","                \n","    return patches\n","\n","def get_positional_embeddings(sequence_length, token_lenght):\n","    result = torch.ones(sequence_length, token_lenght)\n","    for i in range(sequence_length):\n","        for j in range(token_lenght):\n","            result[i][j] = np.sin(i / (10000 ** (j / token_lenght))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / token_lenght)))\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","hidth = 28\n","witdh = 28\n","chann= 1\n","number_of_mages =50\n","desired_token_lenght= 8 \n","number_patches=7\n","patch_size = hidth//number_patches\n","\n","fake_images = torch.rand(number_of_mages, chann, hidth, witdh)\n","cls_t= nn.Parameter(torch.rand(1,desired_token_lenght)) \n","\n","patches=patchify(fake_images,number_patches)         \n","linear_mapp = nn.Linear((hidth//number_patches)**2, desired_token_lenght)    # in order to initialize and tokenize with random weights, pixels in each of patches to desired token lenghth\n","tokens = linear_mapp(patches)                                                # tokenize the input \n","\n","print(patches.shape)\n","print(tokens.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# adding cls_t to each of tokens // vstack = 1x8 to 49x8 => 50x8\n","token_stack = torch.stack([torch.vstack((cls_t, tokens[i])) for i in range(len(tokens))]) \n","print(token_stack.shape)\n","\n","get_pos_em=torch.tensor(get_positional_embeddings(50, 8))\n","print(get_pos_em.shape)\n","\n","pos_embed = nn.Parameter(get_pos_em)\n","print(pos_embed.shape)\n","\n","pos_embed = pos_embed.repeat(50,1,1)\n","print(pos_embed[23,1])\n","\n","out=token_stack+pos_embed\n","\n","out_numpy = out.cpu().detach().numpy()\n","\n","#plt.imshow(fake_images[2,0,:,:])\n","#plt.imshow(out_numpy[2,:,:])\n","\n","'''\n","print(f\"fake images shape     : {fake_images.shape}\") #fake images shape     : torch.Size([5, 1, 28, 28])\n","print(f\"fake images length     : {len(fake_images)}\")  #fake images shape     : 5\n","print(f\"shape of patches      : {patches.shape}\")        #shape of patches      : torch.Size([5, 49, 16])\n","print(f\"shape of tokens       : {tokens.shape}\")       #shape of tokens       : torch.Size([5, 49, 8])\n","print(f\"shape of token_stack  : {token_stack.shape}\")        #shape of token_stack  : torch.Size([5, 50, 8])\n","print(f\"shape of pos_embed    : {pos_embed.shape}\")            #shape of pos_embed    : torch.Size([5, 50, 8])\n","print(f\"shape of out          : {out.shape}\")                  #shape of out          : torch.Size([5, 50, 8])\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","x=torch.randn(7,50,8)\n","models = MyViTBlock(hidden_d=8,n_heads=2)\n","print(f'model output shape',models(x).shape)"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"2wB7viGvf_Tu"},"outputs":[],"source":["class MyViT(nn.Module):\n","    def __init__(self,chw=(1,28,28),n_patches=7, n_blocks=2, token_lenght=8, n_heads=2):  \n","        super(MyViT,self).__init__()\n","        self.chw       = chw\n","        self.n_patches = n_patches\n","        self.token_lenght  = token_lenght\n","        self.n_heads   = n_heads\n","        self.n_blocks  = n_blocks\n","\n","        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"        \n","        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","\n","        self.patch_size   = (chw[1] / n_patches , chw[2] / n_patches )      # patch_size    = 28/7, 28/7  --- 4x4\n","       \n","        #linear mapper \n","        self.input_d      = (chw[0]*self.patch_size[0]*self.patch_size[1])  # input_d       = 1*4*4       --- 16  \n","        self.linear_mapper = nn.Linear(self.input_d, self.token_lenght)         # linear_mapper = input=16, output=8\n","        # classification token \n","        self.class_token = nn.Parameter(torch.rand(1,self.token_lenght))        # class_token 1x8\n","        # positional embedding\n","        self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2+1, self.token_lenght)))\n","        self.pos_embed = requires_grad = False\n","        self.blocks = nn.ModuleList([MyViTBlock(token_lenght,n_heads)for _ in range(n_blocks)])\n","    \n","    def forward(self, images):\n","      n,c,h,w = images.shape   \n","      \n","      patches = patchify(images, self.n_patches) # number of images in dataset x 49 x 16 \n","      tokens = self.linear_mapper(patches)       # number of images in dataset x 49 x 8 \n","\n","      #adding classification tokens\n","      tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))]) \n","\n","      #positional embeding adding\n","      pos_embed = self.pos_embed.repeat(n,1,1)\n","      out = tokens + pos_embed    # (NX50X8)\n","\n","      #transformer Blocks\n","      for block in self.blocks:\n","        out=block(out)\n","\n","      return out"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"SDZqaHZkfC06"},"outputs":[],"source":["class MyViTBlock(nn.Module):\n","    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n","        super().__init__()\n","        self.hidden_d = hidden_d\n","        self.n_heads = n_heads\n","\n","        self.norm1 = nn.LayerNorm(hidden_d)\n","        self.mhsa = MyMSA(hidden_d, n_heads)\n","        self.norm2 = nn.LayerNorm(hidden_d)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","\n","    def forward(self, x):\n","        out = x + self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(out))\n","        return out"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"hd9rx2qPe89n"},"outputs":[],"source":["class MyMSA(nn.Module):\n","  \n","    def __init__(self,d,n_heads=2):\n","      super().__init__()\n","      self.d=d\n","      self.n_heads=n_heads\n","\n","      assert d % n_heads == 0,f\"Can not divide dimension {d}into {n_heads}\"\n","\n","      d_head = int(d/n_heads)\n","      self.q_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.k_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.v_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.d_head=d_head\n","      self.softmax=nn.Softmax(dim=1)\n","\n","    def forward(self,sequences):\n","      result=[]\n","      for sequence in sequences:\n","        seq_result=[]\n","        for head in range (self.n_heads):\n","          q_mapping = self.q_mappings[head]\n","          k_mapping = self.k_mappings[head]\n","          v_mapping = self.v_mappings[head]\n","\n","          seq = sequence[:, head*self.d_head: (head + 1) * self.d_head]\n","          q,k,v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","          attention = self.softmax(q @ k.T / (self.d_head**0.5))\n","          seq_result.append(attention @ v )\n","        result.append(torch.hstack(seq_result))\n","      return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"kr8O4i8KW6h2"},"outputs":[],"source":["def main():\n","    transform =ToTensor()\n","    train_set=MNIST(root='./datasets', train=True, download=True, transform=transform)\n","    test_set=MNIST(root='./datasets', train=False, download=True, transform=transform)\n","    \n","    train_loader = DataLoader(train_set,shuffle=True,batch_size=128)\n","    test_loader = DataLoader(test_set,shuffle=False,batch_size=128)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","    model=...\n","    N_EPOCHS = 5\n","    LR = 0.005\n","    \n","   #training loop\n","    optimizer = Adam(model.parameters(), lr=LR)\n","    criterion = CrossEntropyLoss()\n","    \n","    for epoch in tqdm(range(N_EPOCHS), desc='Training'):\n","        train_loss = 0.00\n","        for batch in tqdm (train_loader, desc=f\"Epoch {epoch+1} in training\", leave=False):\n","            x,y = batch\n","            x,y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat,y)\n","            \n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","        print(f\"Epoch {epoch +1}/{N_EPOCHS}, loss : {train_loss:.2f}\")\n","        \n","    #test loop   \n","    with torch.no_grad():\n","        correct, total =0,0\n","        test_loss = 0\n","        for batch in tqdm(test_loader, desc='Testing'):\n","            x,y     = batch \n","            x,y     = x.to(device), y.to(device)\n","            y_hat   = model(x)\n","            loss    = criterion(y_hat,y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","            \n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","        \n","        print (f\"Test loss : {test_loss:.2f}\")\n","        print (f\"Test accuracy : {correct/total*100:.2f}\")\n"]},{"cell_type":"code","execution_count":149,"metadata":{"id":"M5Q-gBUWONUJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"954313f1a5a7408dacdf77722a4ec770","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b580145bee249ffb9d09e2a8ab73fbf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91556c14529f418c911d35eeaa5a4a5e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f636320cfe074a25a5ba5c557fe99c08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n"]},{"ename":"AttributeError","evalue":"'ellipsis' object has no attribute 'parameters'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion Trasformer.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main()\n","\u001b[1;32m/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion Trasformer.ipynb Cell 11\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m  LR \u001b[39m=\u001b[39m \u001b[39m0.005\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#training loop\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m  optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39mLR)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m  criterion \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/Vision_Transformer/Vsion%20Trasformer.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m  \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(N_EPOCHS), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m'\u001b[39m):\n","\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'parameters'"]}],"source":["if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIzziHRuOXb6"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
