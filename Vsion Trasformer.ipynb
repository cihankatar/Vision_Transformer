{"cells":[{"cell_type":"markdown","metadata":{"id":"a0v9-lkx-S40"},"source":["# **Vision Transformer**\n","CODE:https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/vit_torch.py\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":92,"metadata":{"id":"FgTVi4Fb9Nvj"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn \n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import ToTensor\n","from torchvision.datasets.mnist import MNIST\n","\n","np.random.seed(0)\n","\n","def patchify (images, n_patches):\n","    n,c,h,w = images.shape              #50x1x28x28\n","    \n","    assert h==w,\"Patchify method is implemented for square images only\"   #if 28==28\n","    patches = torch.zeros(n,n_patches**2,(h//n_patches)**2)               #50x49x16\n","    patch_size = h//n_patches\n","    \n","    for idx, image in enumerate(images):\n","        for i in range (n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] # patching for one image \n","                patches[idx, i*n_patches+j]=patch.flatten()     # taking all (49) patches one by one and assigning the patch , first image first patch = 1x16, first image second patch =1x16\n","                \n","    return patches\n","\n","def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n","    return result"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([49, 16])\n"]}],"source":["\n","patches = torch.zeros(50,49,16)   \n","print(patches[1,7])"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([50, 49, 16])\n","torch.Size([50, 49, 8])\n"]}],"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","hidth = 28\n","witdh = 28\n","chann= 1\n","number_of_mages =50\n","desired_token_lenght= 8 \n","number_patches=7\n","patch_size = hidth//number_patches\n","\n","fake_images = torch.rand(number_of_mages, chann, hidth, witdh)\n","cls_t= nn.Parameter(torch.rand(1,desired_token_lenght)) \n","\n","patches=patchify(fake_images,number_patches)         \n","linear_mapp = nn.Linear((hidth//number_patches)**2, desired_token_lenght)    # in order to initialize and tokenize with random weights, pixels in each of patches to desired token lenghth\n","tokens = linear_mapp(patches)                                                # tokenize the input \n","\n","print(patches.shape)\n","print(tokens.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wB7viGvf_Tu"},"outputs":[],"source":["class MyViT(nn.Module):\n","    def __init__(self,chw=(1,28,28),n_patches=7, n_blocks=2, token_lenght=8, n_heads=2):  \n","        super(MyViT,self).__init__()\n","        self.chw       = chw\n","        self.n_patches = n_patches\n","        self.token_lenght  = token_lenght\n","        self.n_heads   = n_heads\n","        self.n_blocks  = n_blocks\n","\n","        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"        \n","        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","\n","        self.patch_size   = (chw[1] / n_patches , chw[2] / n_patches )      # patch_size    = 28/7, 28/7  --- 4x4\n","       \n","        #linear mapper \n","        self.input_d      = (chw[0]*self.patch_size[0]*self.patch_size[1])  # input_d       = 1*4*4       --- 16  \n","        self.linear_mapper = nn.Linear(self.input_d, self.token_lenght)         # linear_mapper = input=16, output=8\n","        # classification token \n","        self.class_token = nn.Parameter(torch.rand(1,self.token_lenght))        # class_token 1x8\n","        # positional embedding\n","        self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2+1, self.token_lenght)))\n","        self.pos_embed = requires_grad = False\n","        self.blocks = nn.ModuleList([MyViTBlock(token_lenght,n_heads)for _ in range(n_blocks)])\n","    \n","    def forward(self, images):\n","      n,c,h,w = images.shape   \n","      \n","      patches = patchify(images, self.n_patches) # number of images in dataset x 49 x 16 \n","      tokens = self.linear_mapper(patches)       # number of images in dataset x 49 x 8 \n","\n","      #adding classification tokens\n","      tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))]) \n","\n","      #positional embeding adding\n","      pos_embed = self.pos_embed.repeat(n,1,1)\n","      out = tokens + pos_embed    # (NX50X8)\n","\n","      #transformer Blocks\n","      for block in self.blocks:\n","        out=block(out)\n","\n","      return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hd9rx2qPe89n"},"outputs":[],"source":["class MyMSA(nn.Module):\n","  \n","    def __init__(self,d,n_heads=2):\n","      super().__init__()\n","      self.d=d\n","      self.n_heads=n_heads\n","\n","      assert d % n_heads == 0,f\"Can not divide dimension {d}into {n_heads}\"\n","\n","      d_head = int(d/n_heads)\n","      self.q_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.k_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.v_mappings=nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)])\n","      self.d_head=d_head\n","      self.softmax=nn.Softmax(dim=1)\n","\n","    def forward(self,sequences):\n","      result=[]\n","      for sequence in sequences:\n","        seq_result=[]\n","        for head in range (self.n_heads):\n","          q_mapping = self.q_mappings[head]\n","          k_mapping = self.k_mappings[head]\n","          v_mapping = self.v_mappings[head]\n","\n","          seq = sequence[:, head*self.d_head: (head + 1) * self.d_head]\n","          q,k,v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","          attention = self.softmax(q @ k.T / (self.d_head**0.5))\n","          seq_result.append(attention @ v )\n","        result.append(torch.hstack(seq_result))\n","      return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDZqaHZkfC06"},"outputs":[],"source":["class MyViTBlock(nn.Module):\n","    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n","        super().__init__()\n","        self.hidden_d = hidden_d\n","        self.n_heads = n_heads\n","\n","        self.norm1 = nn.LayerNorm(hidden_d)\n","        self.mhsa = MyMSA(hidden_d, n_heads)\n","        self.norm2 = nn.LayerNorm(hidden_d)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","\n","    def forward(self, x):\n","        out = x + self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(out))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"n0krmcoCfV1A"},"source":["#**FAKE_IMAGES_TRYING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hmSCqRb-eMQ"},"outputs":[],"source":["\n","# adding cls_t to each of tokens // vstack = 1x8 to 49x8 => 50x8\n","token_stack = torch.stack([torch.vstack((cls_t, tokens[i])) for i in range(len(tokens))]) \n","\n","pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(50, 8)))\n","pos_embed = pos_embed.repeat(5,1,1)\n","\n","out=token_stack+pos_embed\n","\n","out_numpy = out.cpu().detach().numpy()\n","\n","#plt.imshow(fake_images[2,0,:,:])\n","#plt.imshow(out_numpy[2,:,:])\n","\n","print(f\"fake images shape     : {fake_images.shape}\") #fake images shape     : torch.Size([5, 1, 28, 28])\n","print(f\"fake images shape     : {len(fake_images)}\")  #fake images shape     : 5\n","print(f\"shape of patches      : {patches.shape}\")     #shape of patches      : torch.Size([5, 49, 16])\n","print(f\"shape of tokens       : {tokens.shape}\")      #shape of tokens       : torch.Size([5, 49, 8])\n","print(f\"shape of token_stack  : {type(token_stack)}\") #shape of token_stack  : torch.Size([5, 50, 8])\n","print(f\"shape of pos_embed    : {type(pos_embed)}\")   #shape of pos_embed    : torch.Size([5, 50, 8])\n","print(f\"shape of out          : {type(out)}\")         #shape of out          : torch.Size([5, 50, 8])\n","\n","x=torch.randn(70,50,8)\n","models = MyViTBlock(hidden_d=8,n_heads=2)\n","print(f'model output shape',models(x).shape)\n","\n","'''\n","q_mappings=nn.ModuleList([nn.Linear(4,4) for _ in range(2)])\n","k_mappings=nn.ModuleList([nn.Linear(4,4) for _ in range(2)])\n","v_mappings=nn.ModuleList([nn.Linear(4,4) for _ in range(2)])\n","softmax=nn.Softmax(dim=1)\n","\n","result=[]\n","for sequence in out:\n","  seq_result=[]\n","  for head in range (2):            # 2 head for each 50x8\n","    q_mapping = q_mappings[head]\n","    k_mapping = k_mappings[head]\n","    v_mapping = v_mappings[head]\n","\n","    seq = sequence[:, head*4: (head + 1) * 4]               # seq=50x4\n","    q,k,v = q_mapping(seq), k_mapping(seq), v_mapping(seq)  # q = 50x4 after linear(4x4)\n","    attention = softmax(q @ k.T / (2**0.5))\n","    seq_result.append(attention @ v )                       # 2 heads x50x4  \n","  result.append(torch.hstack(seq_result))                   # storing all number of images (nx50x8) n=5\n"," \n","print(f'sequcens_result : ', len(seq_result[0][0]))\n","print(f'result',len(result[0][0]))\n","\n","sonc=torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n","sonc.size()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kr8O4i8KW6h2"},"outputs":[],"source":["def main():\n","    transform =ToTensor()\n","    train_set=MNIST(root='./datasets', train=True, download=True, transform=transform)\n","    test_set=MNIST(root='./datasets', train=False, download=True, transform=transform)\n","    \n","    train_loader = DataLoader(train_set,shuffle=True,batch_size=128)\n","    test_loader = DataLoader(test_set,shuffle=False,batch_size=128)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","    model=...\n","    N_EPOCHS = 5\n","    LR = 0.005\n","    \n","   #training loop\n","    optimizer = Adam(model.parameters(), lr=LR)\n","    criterion = CrossEntropyLoss()\n","    \n","    for epoch in tqdm(range(N_EPOCHS), desc='Training'):\n","        train_loss = 0.00\n","        for batch in tqdm (train_loader, desc=f\"Epoch {epoch+1} in training\", leave=False):\n","            x,y = batch\n","            x,y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat,y)\n","            \n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","        print(f\"Epoch {epoch +1}/{N_EPOCHS}, loss : {train_loss:.2f}\")\n","        \n","    #test loop   \n","    with torch.no_grad():\n","        correct, total =0,0\n","        test_loss = 0\n","        for batch in tqdm(test_loader, desc='Testing'):\n","            x,y     = batch \n","            x,y     = x.to(device), y.to(device)\n","            y_hat   = model(x)\n","            loss    = criterion(y_hat,y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","            \n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","        \n","        print (f\"Test loss : {test_loss:.2f}\")\n","        print (f\"Test accuracy : {correct/total*100:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5Q-gBUWONUJ"},"outputs":[],"source":["if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIzziHRuOXb6"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
