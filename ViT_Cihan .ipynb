{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORT \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PATCHFY FUNCTION & POSITIONAL_EMB\n",
    "\n",
    "def patching_func(images,patch_size):\n",
    "\n",
    "    n,c,h_iamge,w_image=images.shape\n",
    "    n_patch=0\n",
    "    n_patch_per_img = int((h_iamge/patch_size)**2)\n",
    "    patch_vector=int(patch_size**2)\n",
    "    images=images.squeeze()\n",
    "\n",
    "    total_patches = np.zeros((n,n_patch_per_img,patch_vector))\n",
    " \n",
    "    for idx,image in enumerate(images):\n",
    "        patching = patchify(image, (patch_size,patch_size),step=4) # split image into 7x7 small 4x4 patches.patchify(image, (3, 3), step=1)\n",
    "        for i in range(patching.shape[0]):\n",
    "            for j in range (patching.shape[1]):\n",
    "                single_patch=patching[i,j,:,:]\n",
    "                total_patches[idx,n_patch,:]=single_patch.flatten()\n",
    "                n_patch=n_patch+1\n",
    "                if n_patch==49:\n",
    "                    n_patch=0\n",
    "    return total_patches\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATA\n",
    "# \n",
    "transform = ToTensor()\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 1, 28, 28)\n",
      "(96, 49, 16)\n"
     ]
    }
   ],
   "source": [
    "#DATA TO PATCH\n",
    "for batch in train_loader:\n",
    "    images, targets = batch \n",
    "\n",
    "#plt.imshow(images[0].squeeze())      #1x28x28--> 28x28\n",
    "images=images.numpy()\n",
    "\n",
    "print(images.shape)\n",
    "print(patching_func(images,4).shape)\n",
    "patches=torch.tensor(patching_func(images,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((4,3,5))\n",
    "print(x)\n",
    "print(x[:,0])\n",
    "print(x[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSES VIT\n",
    "\n",
    "class ViT (nn.module):\n",
    "    def __init__(self, images_shape, patch_size=4, t_blocks=2, token_dim=64, n_heads=2, output_dim=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        n,c,h_image,w_image=images_shape\n",
    "        self.patch_size = patch_size\n",
    "        self.t_blocks   = t_blocks\n",
    "        self.n_heads    = n_heads\n",
    "        self.token_dim  = token_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        linear_map=nn.Linear(self.patch_size**2,token_dim)\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.token_dim))\n",
    "        self.blocks = nn.ModuleList([ViTBlock(token_dim, n_heads) for _ in range(t_blocks)])\n",
    "\n",
    "        self.linear_classifier  = nn.Linear(self.token_dim, output_dim),\n",
    "        output_pr = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        patches = patching_func(images, self.patc_size)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        out = out[:, 0, :]\n",
    "        out = self.output_pr(self.linear_classifier(out))\n",
    "        return out \n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, token_dim, num_heads, mlp_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp_ratio=mlp_ratio\n",
    "        self.norm1 = norm_layer(token_dim)\n",
    "        self.msa = MSA_Module(token_dim, num_heads)\n",
    "        self.norm2 = norm_layer(token_dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "            nn.Linear(token_dim, self.mlp_ratio * token_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.mlp_ratio * token_dim, token_dim)\n",
    "            )\n",
    "\n",
    "        act_layer=nn.GELU()\n",
    "        norm_layer=nn.LayerNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
